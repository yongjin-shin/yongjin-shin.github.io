<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.5 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Paper Review - Such a Nomad Life</title>
<meta name="description" content="Nash Equilibrium and Multiagent RL">


  <meta name="author" content="Yongjin Shin">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Such a Nomad Life">
<meta property="og:title" content="Paper Review">
<meta property="og:url" content="http://localhost:4000/review/NE-and-MARL/">


  <meta property="og:description" content="Nash Equilibrium and Multiagent RL">







  <meta property="article:published_time" content="2019-09-08T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/review/NE-and-MARL/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Yongjin Shin",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Such a Nomad Life Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Such a Nomad Life
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="https://yongjin-shin.github.io/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/year-archive/" >Posts</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/bio-photo.png" alt="Yongjin Shin" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yongjin Shin</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>I am a grad student at KAIST.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Daejeon in S.Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:yj.shin@kaist.ac.kr" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/yongjin-shin-01309b20" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Linkedin</a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/yongjin-shin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Paper Review">
    <meta itemprop="description" content="An Analysis of Stochastic Game Theory for Multiagent RL (M.Bowling et al. 2000)">
    <meta itemprop="datePublished" content="September 08, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Paper Review
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="an-analysis-of-stochastic-game-theory-for-multiagent-rl-mbowling-et-al-2000">An Analysis of Stochastic Game Theory for Multiagent RL (M.Bowling et al. 2000)</h2>

<ul>
  <li>There is not likely to be an optimal solution to a stochastic game that is indep. of the other agents.</li>
  <li>The algorithms differ on what assumptions they make about the SG and the learning process.
    <ul>
      <li>The main differences are whether a model for the game is available and the nature of the learning process.
        <ul>
          <li>Most GT algorithms require a model of the environment, since they make use of the transition, T, and reward <script type="math/tex">R_i</script>, functions.
            <ul>
              <li>The goal of these algorithms is to compute the quilibrium value for the game rather than finding equilibrium policies.</li>
              <li>“Value” operator refers to algirhtms for solving matrix game, either LP or QP.</li>
            </ul>
          </li>
          <li>RL assume the world is not known, and ony observations of the T and R functions are available as the agents act in the environment.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<ol>
  <li>
    <p><strong>Solutions from Game Theory</strong>: These algorithms learn a value function over states, <script type="math/tex">V(s)</script>. The goal is for <script type="math/tex">V</script> to converge to the optimal value function <script type="math/tex">V*</script>.</p>

    <ul>
      <li>Shapley: <em>Zero-sum SGs</em>. Uses a TD learning to backup values fo next states into a simple matrix game, <script type="math/tex">G_{s}(V)</script>. Nearly identical to value iteration for MDPs, with the “max” operator replaced by the “Value” operator.</li>
      <li>Pollatschek &amp; Avi-Itzhak: Introduced an extension of policy iteration. Each player selects the quilibrium policy according to the current value function, makeing use  fo the same TD matrix <script type="math/tex">G_{S}(V)</script>.
        <ul>
          <li>cons: Only guaranteed to converge if the transition function T and discount factor <script type="math/tex">\gamma</script> satisfies a certain property.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Solutions from RL</strong>: The agents are required to act in the environment in order to gain observations of <script type="math/tex">T</script> and <script type="math/tex">R</script>. These algorithms focus on the behavior of a singe agent, and seek to find the quilibrium policy for that agent.</p>

    <ul>
      <li>Minimax-Q: Extended the traditional Q-learning algorithms for MDPs to <em>zero-sum SG</em>. It replaces the “max”operator with the “Value” operator. This is the off-policy RL equivalent of Shapley’s “value iteration” algorithm.
        <ul>
          <li>cons: converge to the SG’s equilibrium solution, assuming the other agent executes all of its actions infinitely often.</li>
        </ul>
      </li>
      <li>Nash-Q: Extended the Minimax-Q to <em>general-sum algirhtm</em>. Requires that each agent maintain Q values for all the other agents. LP solution is replaced with the QP solution.
        <ul>
          <li>cons: the game must have a unique equilibrium.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/images/2019-09-08-NE-and-MARL/table_GT_RL.png" alt="table_GT_RL" /></p>
  </li>
  <li>
    <p><strong>Others</strong></p>

    <ul>
      <li>Fictitous Play
        <ul>
          <li>assumes opponents play stationary strategies.</li>
          <li>maintains information about the average value of each action</li>
          <li>then deterministically selects the action that has done the best in the past.</li>
          <li><em>nearly identical to single agent value iteration with a uniform weighting of past experience</em></li>
        </ul>
      </li>
      <li>Opponent Modeling/Joint action learners(JAL)
        <ul>
          <li>similar behavior to fictitous play.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="multiagent-rl-for-multi-robot-systems-a-survey-e-yang-et-al-2004">Multiagent RL for multi-robot systems: A Survey (E Yang et al. 2004)</h2>

<p><strong>Frameworks</strong></p>

<ol>
  <li>
    <p><strong>SG-Based</strong></p>

    <p>pros)</p>

    <p>cons) Always require to converge to desirable equilibria, which means sufficient exploration of strategy space is needed before convergence can be established. Agents have to find and even identify the equilibria before the policy is used at the current state.</p>

    <ul>
      <li>
        <p>Algorithms</p>

        <ul>
          <li>
            <p>Minimax-Q-learning</p>
          </li>
          <li>
            <p>Nash-Q-learning</p>
          </li>
          <li>
            <p>Friend-or-Foe Q-learning</p>
          </li>
          <li>
            <p>rQ-learning</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Fictitous Play</strong></p>

    <p>pros) Capable of finding equilibrium in both zeor-sum and general-sum games</p>

    <p>cons) adopts deterministic policies and cannot play stochastic strategies</p>

    <ul>
      <li>
        <p>Algorithms</p>

        <ul>
          <li>JAL</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Bayesian</strong></p>

    <p>pros) Agent can use priors to reason about how its action will influence the behaviors of other games.</p>

    <p>cons) Some prior density over possible dynamics and reward distirbution ahve to be known by a learning agent in advance.</p>
  </li>
  <li>
    <p><strong>Policy Iteration</strong></p>

    <p>pros) Can provide a direct way to find the optimal strategy in the policy space.</p>

    <ul>
      <li>Algorithms
        <ul>
          <li>WoLF-PHC: complete proof for the convergence properties has not been provied. non-markovian environment are not taken into account at all.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="a-comprehensive-survey-of-multi-agent-reinforcement-learning-lbusoniu-et-al-2008">A comprehensive survey of multi-agent reinforcement learning (L.Busoniu et al. 2008)</h2>

<ul>
  <li>
    <p><strong>Challenges in MARL</strong></p>

    <ol>
      <li>The exponential growth of the discrete state-action space in the number of state and action variables
        <ul>
          <li>the curse of dimensionality</li>
        </ul>
      </li>
      <li>
        <p>Specifying a good MARL goal in the general stochastic game</p>

        <ul>
          <li>stability: convergence to a stationary policy</li>
          <li>adaptation: performance is maintained/improved as the other agents are changing their policies</li>
          <li><img src="/assets/images/2019-09-08-NE-and-MARL/stability_vs_adaptation.png" alt="stability_vs_adaptation" /></li>
        </ul>
      </li>
      <li>
        <p>Nonstationarity of the multi-agent learning problem</p>

        <ul>
          <li>each agent is faced with a moving-taget learning problem</li>
        </ul>
      </li>
      <li>
        <p>Exploration-Exploitation trade-off</p>

        <ul>
          <li>need to obtain information not only about the environment but also about the other agents</li>
          <li>too much exploration can destabilize the learning dynamics</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Texanomy</strong></p>

    <ul>
      <li>
        <p>Task Types</p>

        <ul>
          <li><img src="/assets/images/2019-09-08-NE-and-MARL/marl_texanomy.png" alt="marl_texanomy" /></li>
          <li>Mixed Task
            <ul>
              <li>Single-agent RL</li>
              <li>Agent-independent methods
                <ul>
                  <li>Nash-Q learning</li>
                  <li>Correlated Equilibrium Q learning (CE-Q)</li>
                </ul>
              </li>
              <li>Agent-tracking methods: Convergence to stationary strategies is not a requirement.
                <ul>
                  <li>Static: fictitious play, MetaStrategy, Hyper-Q</li>
                  <li>Dynamic: Non-Stationary Converging Policies(NSCP),</li>
                </ul>
              </li>
              <li>Agent-aware methods: target convergence, as well as adaptation to the other agents.
                <ul>
                  <li>Static: AWESOME, IGA/WoLF-IGA/GIGA/GIGA-WoLF,</li>
                  <li>Dynamic: WoLF-PHC, EXORL</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>

        <p><em>–&gt; many algorithms for mixed SGs suffer from the curse of dimensionality, and are sensitive to imperfect observations; the latter holds especially for agent-independent methods.</em></p>
      </li>
      <li>
        <p>Field of origin</p>
        <ul>
          <li><img src="/assets/images/2019-09-08-NE-and-MARL/field%20of%20origin.png" alt="field of origin" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="deep-rl-for-mas-a-review-of-challenges-solutions-and-applications-tnguyen-et-al-2019">Deep RL for MAS: A Review of Challenges, Solutions and Applications (T.Nguyen et al, 2019)</h2>

<ul>
  <li>Challenges
    <ul>
      <li>Non-stationarity</li>
      <li>Partial observability</li>
      <li>MAS training schemes</li>
      <li>Continous action spaces</li>
      <li>Transfer Learning</li>
      <li><img src="/assets/images/2019-09-08-NE-and-MARL/madrl_texanomy.png" alt="madrl_texanomy" /></li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#gt" class="page__taxonomy-item" rel="tag">GT</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#rl" class="page__taxonomy-item" rel="tag">RL</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#review" class="page__taxonomy-item" rel="tag">review</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-09-08T00:00:00+09:00">September 08, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Paper+Review%20http%3A%2F%2Flocalhost%3A4000%2Freview%2FNE-and-MARL%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Freview%2FNE-and-MARL%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Freview%2FNE-and-MARL%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/rl/actor-critic-and-policy-gradients/" class="pagination--pager" title="Actor Critic And Policy Gradients
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/rl/actor-critic-and-policy-gradients/" rel="permalink">Actor Critic And Policy Gradients
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/research/talk/" rel="permalink">좋은 연구란
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">고맙게도 연구실원 중에 한 친구가 가끔씩 커피를 마시러 가자고 얘기를 해준다.
오늘도 논문 보면서 심란해하다가 이야기를 나누게 되었다.
다들 연차가 쌓여있는 친구들이라서 이해 안 되는 이야기를 할 때가 많지만, 그래도 도움이 되는 내용들이 항상 존재한다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/mas/game%20theory/lp-gt1/" rel="permalink">An Intro Lin.Prog &amp; Game Theory-Chapter8
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Multi-agent Systems의 극악무도한 내용 때문에 다른 강의들 뒤적이다가 뭔가 LP를 다시 봐야할 것 같아서, 예전에 대학원 시험 때문에 공부했던 “An Introduction Linear Programming and Game Theory”를 다시 펼쳐봤습니다..신세계를...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/book/mas-chap4/" rel="permalink">Multi-agent Systems-Chapter4
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">3장에서는 게임에 있어서 최적 솔루션이 무엇인가(혹은 Nash Euilibrium) 이외에 다른 대안이 없는지에 대해서다. 4장에서는 그 솔루션을 얻는데 있어 계산이 얼마나 복잡할 것인가에 대한 문제를 다루게 된다. 자연스럽게 가장 간단한 2명의 플레이어, 제로섬 normal-fo...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/yongjin-shin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Yongjin Shin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'yongjin-shin/yongjin-shin.github.io');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




  </body>
</html>
