<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.5 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Actor Critic And Policy Gradients - Such a Nomad Life</title>
<meta name="description" content="논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)">


  <meta name="author" content="Yongjin Shin">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Such a Nomad Life">
<meta property="og:title" content="Actor Critic And Policy Gradients">
<meta property="og:url" content="http://localhost:4000/rl/actor-critic-and-policy-gradients/">


  <meta property="og:description" content="논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)">







  <meta property="article:published_time" content="2019-09-06T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/rl/actor-critic-and-policy-gradients/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Yongjin Shin",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Such a Nomad Life Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Such a Nomad Life
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="https://yongjin-shin.github.io/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/year-archive/" >Posts</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/bio-photo.png" alt="Yongjin Shin" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yongjin Shin</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>I am a grad student at KAIST.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Daejeon in S.Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:yj.shin@kaist.ac.kr" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/yongjin-shin-01309b20" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Linkedin</a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/yongjin-shin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Actor Critic And Policy Gradients">
    <meta itemprop="description" content="논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)">
    <meta itemprop="datePublished" content="September 06, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Actor Critic And Policy Gradients
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)</p>

<h2 id="vanilla-policy-gradient">Vanilla Policy Gradient</h2>

<p>RL에서 하고자 하는 것은 어떠한 tast에서 가장 좋은 결과를 얻기 위한 Action을 찾아내는 것이다. (스탠포드에서는 Sequential Decision Making이라고 했다) 따라서 이를 나타내보자면,</p>

<script type="math/tex; mode=display">\theta^{*} = argmax_{\theta} \mathbb{E}_{\tau} \sim p_{theta}(\tau) \sum_{t} r(s_{t}, a_{t})]</script>

<p>우리가 원하는 것은 <script type="math/tex">\theta</script>의 optimal 값이 되겠다. 여기서 기대값을 구해야 하는데, 가장 쉽게는 Monte Calro를 이용하면 된다. 물론 몬테카를로가 기대값으로 수렴하기 위해서는 충분히 많은 샘플이 있어야 하고, RL에서는 특정 <script type="math/tex">s_0, a_0</script>부터 <script type="math/tex">s_{des}, a_{des}</script>까지 모두 샘플을 얻어야만 한다. (여기서 문제점은 어느 세월에 이러한 샘플링을 할 것인가에 대한 물음이 따르게 된다.) 여튼, 몬테카를로는 하기 싫기 때문에(왜??) Gradient descent/ascent를 하고자 한다. 이걸 도입하는 순간 Global Optimum은 포기한게 아닐까? 뭐, global optimum을 구하면 좋기는 한데 global이고 나발이고 일단 optimum부터 찾는게 문제이니 일단 구해보자. 증명과정 약간의 log 트릭을 사용하면 어째저째 할 수 있으니 생략하고, 결과는</p>

<script type="math/tex; mode=display">\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}(\tau) \big [ \big( \sum_{t=1}^{T}\nabla_{\theta} \log{\pi_{\theta}(a_{t}|s_{t})} \big) \big( \sum_{t=1}^{T} r(s_{t}, a_{t}) \big) \big]</script>

<p>흠, 그런데 어쨌거나 샘플링을 많이 한다음 derivative를 계산하고 <script type="math/tex">\theta</script>를 업데이트 해줘야 한다. <script type="math/tex">(\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta))</script></p>

<table>
  <tbody>
    <tr>
      <td>위의 derivative를 조금 더 음미를 해보자면, 리워드 합의 기대값이 큰 방향으로 <script type="math/tex">\theta</script>를 이동시켜주는 것이다. (마치 로지스틱 리그레션처럼) 근데 위의 수식에서 리워드 합의 기대값  $$\sum_{t=1}^{T} r(a_{t}</td>
      <td>s_{t})<script type="math/tex">부분만 제거해주면,</script>J_{\theta}$$의 MLE와 동일하게 된다. 나중에 텐서플로우에서는 MLE를 잘해준 다음에 각각의 샘플에 리워드 합의 기대값만 weight sum형식으로 진행해주면 된다는 말이다.</td>
    </tr>
  </tbody>
</table>

<h2 id="reducing-variance">Reducing Variance</h2>

<p>근데 문제점은 단순히 derivative 구해서 업데이트를 해주면 variance가 크다는 말. 즉, 와리가리 하다가 세월 다 보내게 될 수도 있다. 내가 예측한 모델에서 미래의 값들은 점점 더 현실에서 멀어지기 때문에, 단순히 리워드 sum을 해버리면 문제가 생길 수 밖에 없다. 위에서 언급했듯이, derivative는 일종의 weighted sum인데 불확실성이 큰 부분들까지 똑같이 weight가 걸리면 오차가 점점 커질 수 밖에 없다. 이걸 해결해줄 수 있는 방법은 weight를 달리 주는 것이다. 이와 관련된 방법은,</p>

<ol>
  <li>리워드 sum을 할 때 과거는 무시하고 앞으로의 일들만 고려하자.</li>
  <li>각 샘플 sequence로부터 평균을 구해서 거기서 얼마나 차이가 나는지를 weight로 사용해보자.</li>
</ol>

<p>결과만 수식으로 나타내보자면</p>

<script type="math/tex; mode=display">\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}(\tau) \big [ \big( \sum_{t=1}^{T}\nabla_{\theta} \log{\pi_{\theta}(a_{t}|s_{t})} \big) \big( \sum_{t'=t}^{T} r(s_{t'}, a_{t'}) - b \big) \big]</script>

<script type="math/tex; mode=display">b = \frac{1}{N}\sum_{i=1}^{N}r(\tau_{i})</script>

<p>(여러가지 수식들로 풀어쓸 수 있지만, 아직 vim이랑 markdown이 익숙하지 않으므로 여기까지만…)</p>

<h2 id="actor-critic-algorithms">Actor-Critic Algorithms</h2>
<p>그런데 위의 식에서 weight 부분은 어쨌거나 estimate이다. 왜냐하면 우리가 원하는 완벽한 <script type="math/tex">\theta^{*}</script>에 의해서 샘플링된 <script type="math/tex">s_{t}, a_{t}</script>가 아니기 때문이다. 뿐만 아니라, 분산을 줄이기 위해서 계산을 할 때 오로지 해당 시점에서의 샘플들만 고려를 하지, 전체 sequence의 샘플을 모두 고려하는 과정이 없다. 그렇기 때문에 biased는 되어있을지라도 여전히 분산은 높은 실정이다. 그렇다면 우리가 앞에서 계산한 것처럼 계산을 해주는 것보다 더 정확하게 계산할 수는 없을까? 어차피 우리는 <script type="math/tex">b</script>로 평균을 구한다. 차라리 Q-function에서 action들까지 평균치를 처리한 value function을 이용하면 어떨까? 즉,  <script type="math/tex">V^{\pi}(s_{t}) = \mathbb{E}_{a_{t} \sim \pi_{0}(a_{t}|s_{t})}[Q_{\pi}(s_{t}|a_{t})]</script>를 사용하기로 해보자. 여기서 Advantage function이 튀어나온다.</p>

<script type="math/tex; mode=display">A^{\pi}(s_{t}, a_{t}) = Q^{\pi}(s_{t}, a_{t}) - V^{\pi}(s_{t})</script>

<p>이것은 weight로 역할을 할 것인데, 평균적인 action들의 값에 비해 특정 action이 얼마나 의미가 있을지에 대한 weight로써 작용을 할 것이다. 자, 이제는 Q는 잘 구하면 되는데, V까지 나타난 시점에서 우리는 모든 action들을 고려한 샘플링을 할 수는 없을 것이다. 따라서 V를 approximation하는 과정을 거칠 것이다. function approximator인 뉴럴넷을 활용하면 될 것이다. 일단 Q는 아래와 같이 근사할 수 있다. 왜냐하면 <script type="math/tex">V^{\pi}</script>가 오로지 샘플링으로 구한 리워드 sum과 완전히 동일하지는 않기 때문이다.</p>

<script type="math/tex; mode=display">Q(s_{t}, a_{t}) = \sum_{t=1}^{T}r(s_{t}, a_{t}) \approx r(s_{t}, a_{t}) + V^{\pi}(s_{t+1})</script>

<p>이때, 위에서 언급한 Advantage function에 쏙 집어넣으면 <script type="math/tex">Q^{pi}</script>는 사라지고 <script type="math/tex">V^{\pi}</script>로만 Advantage function을 나타낼 수 있게 된다.</p>

<script type="math/tex; mode=display">A^{\pi}(s_{t}, a_{t}) \approx r(s_{t}, a_{t}) + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})</script>

<p>따라서, 우리는 <script type="math/tex">V^{\pi}</script>만 잘 파악하면(policy evaluation) bias도 덜하고 variance도 작은 방향으로 <script type="math/tex">\theta</script>를 업데이트(policy improvement)해 줄 수 있는 해피한 상황이 되었다. 하지만 이 V-function을 어떻게 잘 구해줄 수 있을까? 뉴럴넷은 어떻게 학습을 시켜야만 value funtion의 parameter인 <script type="math/tex">\phi</script>를 학습 시킬 수 있을까? 다시금 fitting을 해줘야 하는 상황으로 넘어가게 된다.</p>

<p>답은, supervised learning을 해야한다. 샘플링을 통해 얻은 <script type="math/tex">\{(s_{i,t}, a_{i,t})\}</script>에서 리워드 <script type="math/tex">\{r_{i, t}\}</script>를 구할 수 있다. 또 기존의 value-function으로 estimated reward <script type="math/tex">\{y_{i, t}\}</script>를 구할 수 있다. 결국 흔히 보듯이 L2 norm을 구해버리고 학습을 시키면 된다.</p>

<script type="math/tex; mode=display">\mathit{L}(\phi) = \frac{1}{2}\sum_{i}||V^{\pi}_{\phi}(s_{i}) - y_{i}||^{2}</script>

<p>추가적으로 언급할 부분은 Advantage function을 구성할 때 bias-variance tradeoff 이슈가 생긴다. 만약 우리가 더 많은 variance를 허용할 수 없다면, 우리는 더 많은 실제 샘플을 추가해서 진행하면 된다. 그럴수록 우리는 더 많은 bias가 생길 수 밖에 없다. 이 부분은 <script type="math/tex">TD(\lambda)</script>의 개념을 떠올리면 쉽게 이해가 갈 것이다.</p>

<p>마지막으로 Actor-Critic의 개념은 Actor는 실제로 action을 담담하게 되는 Q-function 쪽을 의미하게 되고, critic은 기준점이 되는 V-function 을 의미하게 된다.</p>

<p>참고&gt;</p>
<ol>
  <li><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic</a></li>
  <li><a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/">https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/</a></li>
  <li><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/">http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/</a></li>
</ol>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#actor-critic-algorithms" class="page__taxonomy-item" rel="tag">Actor-Critic Algorithms</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#policy-gradients" class="page__taxonomy-item" rel="tag">Policy Gradients</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#rl" class="page__taxonomy-item" rel="tag">RL</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-09-06T00:00:00+09:00">September 06, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Actor+Critic+And+Policy+Gradients%20http%3A%2F%2Flocalhost%3A4000%2Frl%2Factor-critic-and-policy-gradients%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Frl%2Factor-critic-and-policy-gradients%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Frl%2Factor-critic-and-policy-gradients%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/research/talk/" class="pagination--pager" title="좋은 연구란
">Previous</a>
    
    
      <a href="/review/NE-and-MARL/" class="pagination--pager" title="Paper Review
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/review/NE-and-MARL/" rel="permalink">Paper Review
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">An Analysis of Stochastic Game Theory for Multiagent RL (M.Bowling et al. 2000)

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/research/talk/" rel="permalink">좋은 연구란
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">고맙게도 연구실원 중에 한 친구가 가끔씩 커피를 마시러 가자고 얘기를 해준다.
오늘도 논문 보면서 심란해하다가 이야기를 나누게 되었다.
다들 연차가 쌓여있는 친구들이라서 이해 안 되는 이야기를 할 때가 많지만, 그래도 도움이 되는 내용들이 항상 존재한다.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/mas/game%20theory/lp-gt1/" rel="permalink">An Intro Lin.Prog &amp; Game Theory-Chapter8
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Multi-agent Systems의 극악무도한 내용 때문에 다른 강의들 뒤적이다가 뭔가 LP를 다시 봐야할 것 같아서, 예전에 대학원 시험 때문에 공부했던 “An Introduction Linear Programming and Game Theory”를 다시 펼쳐봤습니다..신세계를...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/book/mas-chap4/" rel="permalink">Multi-agent Systems-Chapter4
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">3장에서는 게임에 있어서 최적 솔루션이 무엇인가(혹은 Nash Euilibrium) 이외에 다른 대안이 없는지에 대해서다. 4장에서는 그 솔루션을 얻는데 있어 계산이 얼마나 복잡할 것인가에 대한 문제를 다루게 된다. 자연스럽게 가장 간단한 2명의 플레이어, 제로섬 normal-fo...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/yongjin-shin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Yongjin Shin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'yongjin-shin/yongjin-shin.github.io');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




  </body>
</html>
