<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-09-08T16:17:46+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Such a Nomad Life</title><subtitle>Blogging With Minimal Effort..!</subtitle><author><name>Yongjin Shin</name></author><entry><title type="html">Paper Review</title><link href="http://localhost:4000/review/NE-and-MARL/" rel="alternate" type="text/html" title="Paper Review" /><published>2019-09-08T00:00:00+09:00</published><updated>2019-09-08T00:00:00+09:00</updated><id>http://localhost:4000/review/NE-and-MARL</id><content type="html" xml:base="http://localhost:4000/review/NE-and-MARL/">&lt;h2 id=&quot;an-analysis-of-stochastic-game-theory-for-multiagent-rl-mbowling-et-al-2000&quot;&gt;An Analysis of Stochastic Game Theory for Multiagent RL (M.Bowling et al. 2000)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;There is not likely to be an optimal solution to a stochastic game that is indep. of the other agents.&lt;/li&gt;
  &lt;li&gt;The algorithms differ on what assumptions they make about the SG and the learning process.
    &lt;ul&gt;
      &lt;li&gt;The main differences are whether a model for the game is available and the nature of the learning process.
        &lt;ul&gt;
          &lt;li&gt;Most GT algorithms require a model of the environment, since they make use of the transition, T, and reward &lt;script type=&quot;math/tex&quot;&gt;R_i&lt;/script&gt;, functions.
            &lt;ul&gt;
              &lt;li&gt;The goal of these algorithms is to compute the quilibrium value for the game rather than finding equilibrium policies.&lt;/li&gt;
              &lt;li&gt;“Value” operator refers to algirhtms for solving matrix game, either LP or QP.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;RL assume the world is not known, and ony observations of the T and R functions are available as the agents act in the environment.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Solutions from Game Theory&lt;/strong&gt;: These algorithms learn a value function over states, &lt;script type=&quot;math/tex&quot;&gt;V(s)&lt;/script&gt;. The goal is for &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; to converge to the optimal value function &lt;script type=&quot;math/tex&quot;&gt;V*&lt;/script&gt;.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Shapley: &lt;em&gt;Zero-sum SGs&lt;/em&gt;. Uses a TD learning to backup values fo next states into a simple matrix game, &lt;script type=&quot;math/tex&quot;&gt;G_{s}(V)&lt;/script&gt;. Nearly identical to value iteration for MDPs, with the “max” operator replaced by the “Value” operator.&lt;/li&gt;
      &lt;li&gt;Pollatschek &amp;amp; Avi-Itzhak: Introduced an extension of policy iteration. Each player selects the quilibrium policy according to the current value function, makeing use  fo the same TD matrix &lt;script type=&quot;math/tex&quot;&gt;G_{S}(V)&lt;/script&gt;.
        &lt;ul&gt;
          &lt;li&gt;cons: Only guaranteed to converge if the transition function T and discount factor &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; satisfies a certain property.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Solutions from RL&lt;/strong&gt;: The agents are required to act in the environment in order to gain observations of &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;. These algorithms focus on the behavior of a singe agent, and seek to find the quilibrium policy for that agent.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Minimax-Q: Extended the traditional Q-learning algorithms for MDPs to &lt;em&gt;zero-sum SG&lt;/em&gt;. It replaces the “max”operator with the “Value” operator. This is the off-policy RL equivalent of Shapley’s “value iteration” algorithm.
        &lt;ul&gt;
          &lt;li&gt;cons: converge to the SG’s equilibrium solution, assuming the other agent executes all of its actions infinitely often.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nash-Q: Extended the Minimax-Q to &lt;em&gt;general-sum algirhtm&lt;/em&gt;. Requires that each agent maintain Q values for all the other agents. LP solution is replaced with the QP solution.
        &lt;ul&gt;
          &lt;li&gt;cons: the game must have a unique equilibrium.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/2019-09-08-NE-and-MARL/table_GT_RL.png&quot; alt=&quot;table_GT_RL&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Others&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Fictitous Play
        &lt;ul&gt;
          &lt;li&gt;assumes opponents play stationary strategies.&lt;/li&gt;
          &lt;li&gt;maintains information about the average value of each action&lt;/li&gt;
          &lt;li&gt;then deterministically selects the action that has done the best in the past.&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;nearly identical to single agent value iteration with a uniform weighting of past experience&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Opponent Modeling/Joint action learners(JAL)
        &lt;ul&gt;
          &lt;li&gt;similar behavior to fictitous play.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;multiagent-rl-for-multi-robot-systems-a-survey-e-yang-et-al-2004&quot;&gt;Multiagent RL for multi-robot systems: A Survey (E Yang et al. 2004)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Frameworks&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SG-Based&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;pros)&lt;/p&gt;

    &lt;p&gt;cons) Always require to converge to desirable equilibria, which means sufficient exploration of strategy space is needed before convergence can be established. Agents have to find and even identify the equilibria before the policy is used at the current state.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Algorithms&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Minimax-Q-learning&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Nash-Q-learning&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Friend-or-Foe Q-learning&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;rQ-learning&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Fictitous Play&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;pros) Capable of finding equilibrium in both zeor-sum and general-sum games&lt;/p&gt;

    &lt;p&gt;cons) adopts deterministic policies and cannot play stochastic strategies&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Algorithms&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;JAL&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bayesian&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;pros) Agent can use priors to reason about how its action will influence the behaviors of other games.&lt;/p&gt;

    &lt;p&gt;cons) Some prior density over possible dynamics and reward distirbution ahve to be known by a learning agent in advance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Policy Iteration&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;pros) Can provide a direct way to find the optimal strategy in the policy space.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Algorithms
        &lt;ul&gt;
          &lt;li&gt;WoLF-PHC: complete proof for the convergence properties has not been provied. non-markovian environment are not taken into account at all.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;a-comprehensive-survey-of-multi-agent-reinforcement-learning-lbusoniu-et-al-2008&quot;&gt;A comprehensive survey of multi-agent reinforcement learning (L.Busoniu et al. 2008)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Challenges in MARL&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;The exponential growth of the discrete state-action space in the number of state and action variables
        &lt;ul&gt;
          &lt;li&gt;the curse of dimensionality&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Specifying a good MARL goal in the general stochastic game&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;stability: convergence to a stationary policy&lt;/li&gt;
          &lt;li&gt;adaptation: performance is maintained/improved as the other agents are changing their policies&lt;/li&gt;
          &lt;li&gt;&lt;img src=&quot;/assets/images/2019-09-08-NE-and-MARL/stability_vs_adaptation.png&quot; alt=&quot;stability_vs_adaptation&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Nonstationarity of the multi-agent learning problem&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;each agent is faced with a moving-taget learning problem&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Exploration-Exploitation trade-off&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;need to obtain information not only about the environment but also about the other agents&lt;/li&gt;
          &lt;li&gt;too much exploration can destabilize the learning dynamics&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Texanomy&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Task Types&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;img src=&quot;/assets/images/2019-09-08-NE-and-MARL/marl_texanomy.png&quot; alt=&quot;marl_texanomy&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Mixed Task
            &lt;ul&gt;
              &lt;li&gt;Single-agent RL&lt;/li&gt;
              &lt;li&gt;Agent-independent methods
                &lt;ul&gt;
                  &lt;li&gt;Nash-Q learning&lt;/li&gt;
                  &lt;li&gt;Correlated Equilibrium Q learning (CE-Q)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Agent-tracking methods: Convergence to stationary strategies is not a requirement.
                &lt;ul&gt;
                  &lt;li&gt;Static: fictitious play, MetaStrategy, Hyper-Q&lt;/li&gt;
                  &lt;li&gt;Dynamic: Non-Stationary Converging Policies(NSCP),&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Agent-aware methods: target convergence, as well as adaptation to the other agents.
                &lt;ul&gt;
                  &lt;li&gt;Static: AWESOME, IGA/WoLF-IGA/GIGA/GIGA-WoLF,&lt;/li&gt;
                  &lt;li&gt;Dynamic: WoLF-PHC, EXORL&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;em&gt;–&amp;gt; many algorithms for mixed SGs suffer from the curse of dimensionality, and are sensitive to imperfect observations; the latter holds especially for agent-independent methods.&lt;/em&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Field of origin&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;img src=&quot;/assets/images/2019-09-08-NE-and-MARL/field%20of%20origin.png&quot; alt=&quot;field of origin&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-rl-for-mas-a-review-of-challenges-solutions-and-applications-tnguyen-et-al-2019&quot;&gt;Deep RL for MAS: A Review of Challenges, Solutions and Applications (T.Nguyen et al, 2019)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Challenges
    &lt;ul&gt;
      &lt;li&gt;Non-stationarity&lt;/li&gt;
      &lt;li&gt;Partial observability&lt;/li&gt;
      &lt;li&gt;MAS training schemes&lt;/li&gt;
      &lt;li&gt;Continous action spaces&lt;/li&gt;
      &lt;li&gt;Transfer Learning&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/2019-09-08-NE-and-MARL/madrl_texanomy.png&quot; alt=&quot;madrl_texanomy&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yongjin Shin</name></author><category term="RL" /><category term="GT" /><summary type="html">An Analysis of Stochastic Game Theory for Multiagent RL (M.Bowling et al. 2000)</summary></entry><entry><title type="html">Actor Critic And Policy Gradients</title><link href="http://localhost:4000/rl/actor-critic-and-policy-gradients/" rel="alternate" type="text/html" title="Actor Critic And Policy Gradients" /><published>2019-09-06T00:00:00+09:00</published><updated>2019-09-06T00:00:00+09:00</updated><id>http://localhost:4000/rl/actor-critic-and-policy-gradients</id><content type="html" xml:base="http://localhost:4000/rl/actor-critic-and-policy-gradients/">&lt;p&gt;논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)&lt;/p&gt;

&lt;h2 id=&quot;vanilla-policy-gradient&quot;&gt;Vanilla Policy Gradient&lt;/h2&gt;

&lt;p&gt;RL에서 하고자 하는 것은 어떠한 tast에서 가장 좋은 결과를 얻기 위한 Action을 찾아내는 것이다. (스탠포드에서는 Sequential Decision Making이라고 했다) 따라서 이를 나타내보자면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{*} = argmax_{\theta} \mathbb{E}_{\tau} \sim p_{theta}(\tau) \sum_{t} r(s_{t}, a_{t})]&lt;/script&gt;

&lt;p&gt;우리가 원하는 것은 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;의 optimal 값이 되겠다. 여기서 기대값을 구해야 하는데, 가장 쉽게는 Monte Calro를 이용하면 된다. 물론 몬테카를로가 기대값으로 수렴하기 위해서는 충분히 많은 샘플이 있어야 하고, RL에서는 특정 &lt;script type=&quot;math/tex&quot;&gt;s_0, a_0&lt;/script&gt;부터 &lt;script type=&quot;math/tex&quot;&gt;s_{des}, a_{des}&lt;/script&gt;까지 모두 샘플을 얻어야만 한다. (여기서 문제점은 어느 세월에 이러한 샘플링을 할 것인가에 대한 물음이 따르게 된다.) 여튼, 몬테카를로는 하기 싫기 때문에(왜??) Gradient descent/ascent를 하고자 한다. 이걸 도입하는 순간 Global Optimum은 포기한게 아닐까? 뭐, global optimum을 구하면 좋기는 한데 global이고 나발이고 일단 optimum부터 찾는게 문제이니 일단 구해보자. 증명과정 약간의 log 트릭을 사용하면 어째저째 할 수 있으니 생략하고, 결과는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}(\tau) \big [ \big( \sum_{t=1}^{T}\nabla_{\theta} \log{\pi_{\theta}(a_{t}|s_{t})} \big) \big( \sum_{t=1}^{T} r(s_{t}, a_{t}) \big) \big]&lt;/script&gt;

&lt;p&gt;흠, 그런데 어쨌거나 샘플링을 많이 한다음 derivative를 계산하고 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;를 업데이트 해줘야 한다. &lt;script type=&quot;math/tex&quot;&gt;(\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta))&lt;/script&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;위의 derivative를 조금 더 음미를 해보자면, 리워드 합의 기대값이 큰 방향으로 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;를 이동시켜주는 것이다. (마치 로지스틱 리그레션처럼) 근데 위의 수식에서 리워드 합의 기대값  $$\sum_{t=1}^{T} r(a_{t}&lt;/td&gt;
      &lt;td&gt;s_{t})&lt;script type=&quot;math/tex&quot;&gt;부분만 제거해주면,&lt;/script&gt;J_{\theta}$$의 MLE와 동일하게 된다. 나중에 텐서플로우에서는 MLE를 잘해준 다음에 각각의 샘플에 리워드 합의 기대값만 weight sum형식으로 진행해주면 된다는 말이다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;reducing-variance&quot;&gt;Reducing Variance&lt;/h2&gt;

&lt;p&gt;근데 문제점은 단순히 derivative 구해서 업데이트를 해주면 variance가 크다는 말. 즉, 와리가리 하다가 세월 다 보내게 될 수도 있다. 내가 예측한 모델에서 미래의 값들은 점점 더 현실에서 멀어지기 때문에, 단순히 리워드 sum을 해버리면 문제가 생길 수 밖에 없다. 위에서 언급했듯이, derivative는 일종의 weighted sum인데 불확실성이 큰 부분들까지 똑같이 weight가 걸리면 오차가 점점 커질 수 밖에 없다. 이걸 해결해줄 수 있는 방법은 weight를 달리 주는 것이다. 이와 관련된 방법은,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;리워드 sum을 할 때 과거는 무시하고 앞으로의 일들만 고려하자.&lt;/li&gt;
  &lt;li&gt;각 샘플 sequence로부터 평균을 구해서 거기서 얼마나 차이가 나는지를 weight로 사용해보자.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;결과만 수식으로 나타내보자면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}(\tau) \big [ \big( \sum_{t=1}^{T}\nabla_{\theta} \log{\pi_{\theta}(a_{t}|s_{t})} \big) \big( \sum_{t'=t}^{T} r(s_{t'}, a_{t'}) - b \big) \big]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b = \frac{1}{N}\sum_{i=1}^{N}r(\tau_{i})&lt;/script&gt;

&lt;p&gt;(여러가지 수식들로 풀어쓸 수 있지만, 아직 vim이랑 markdown이 익숙하지 않으므로 여기까지만…)&lt;/p&gt;

&lt;h2 id=&quot;actor-critic-algorithms&quot;&gt;Actor-Critic Algorithms&lt;/h2&gt;
&lt;p&gt;그런데 위의 식에서 weight 부분은 어쨌거나 estimate이다. 왜냐하면 우리가 원하는 완벽한 &lt;script type=&quot;math/tex&quot;&gt;\theta^{*}&lt;/script&gt;에 의해서 샘플링된 &lt;script type=&quot;math/tex&quot;&gt;s_{t}, a_{t}&lt;/script&gt;가 아니기 때문이다. 뿐만 아니라, 분산을 줄이기 위해서 계산을 할 때 오로지 해당 시점에서의 샘플들만 고려를 하지, 전체 sequence의 샘플을 모두 고려하는 과정이 없다. 그렇기 때문에 biased는 되어있을지라도 여전히 분산은 높은 실정이다. 그렇다면 우리가 앞에서 계산한 것처럼 계산을 해주는 것보다 더 정확하게 계산할 수는 없을까? 어차피 우리는 &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;로 평균을 구한다. 차라리 Q-function에서 action들까지 평균치를 처리한 value function을 이용하면 어떨까? 즉,  &lt;script type=&quot;math/tex&quot;&gt;V^{\pi}(s_{t}) = \mathbb{E}_{a_{t} \sim \pi_{0}(a_{t}|s_{t})}[Q_{\pi}(s_{t}|a_{t})]&lt;/script&gt;를 사용하기로 해보자. 여기서 Advantage function이 튀어나온다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^{\pi}(s_{t}, a_{t}) = Q^{\pi}(s_{t}, a_{t}) - V^{\pi}(s_{t})&lt;/script&gt;

&lt;p&gt;이것은 weight로 역할을 할 것인데, 평균적인 action들의 값에 비해 특정 action이 얼마나 의미가 있을지에 대한 weight로써 작용을 할 것이다. 자, 이제는 Q는 잘 구하면 되는데, V까지 나타난 시점에서 우리는 모든 action들을 고려한 샘플링을 할 수는 없을 것이다. 따라서 V를 approximation하는 과정을 거칠 것이다. function approximator인 뉴럴넷을 활용하면 될 것이다. 일단 Q는 아래와 같이 근사할 수 있다. 왜냐하면 &lt;script type=&quot;math/tex&quot;&gt;V^{\pi}&lt;/script&gt;가 오로지 샘플링으로 구한 리워드 sum과 완전히 동일하지는 않기 때문이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(s_{t}, a_{t}) = \sum_{t=1}^{T}r(s_{t}, a_{t}) \approx r(s_{t}, a_{t}) + V^{\pi}(s_{t+1})&lt;/script&gt;

&lt;p&gt;이때, 위에서 언급한 Advantage function에 쏙 집어넣으면 &lt;script type=&quot;math/tex&quot;&gt;Q^{pi}&lt;/script&gt;는 사라지고 &lt;script type=&quot;math/tex&quot;&gt;V^{\pi}&lt;/script&gt;로만 Advantage function을 나타낼 수 있게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^{\pi}(s_{t}, a_{t}) \approx r(s_{t}, a_{t}) + V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})&lt;/script&gt;

&lt;p&gt;따라서, 우리는 &lt;script type=&quot;math/tex&quot;&gt;V^{\pi}&lt;/script&gt;만 잘 파악하면(policy evaluation) bias도 덜하고 variance도 작은 방향으로 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;를 업데이트(policy improvement)해 줄 수 있는 해피한 상황이 되었다. 하지만 이 V-function을 어떻게 잘 구해줄 수 있을까? 뉴럴넷은 어떻게 학습을 시켜야만 value funtion의 parameter인 &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;를 학습 시킬 수 있을까? 다시금 fitting을 해줘야 하는 상황으로 넘어가게 된다.&lt;/p&gt;

&lt;p&gt;답은, supervised learning을 해야한다. 샘플링을 통해 얻은 &lt;script type=&quot;math/tex&quot;&gt;\{(s_{i,t}, a_{i,t})\}&lt;/script&gt;에서 리워드 &lt;script type=&quot;math/tex&quot;&gt;\{r_{i, t}\}&lt;/script&gt;를 구할 수 있다. 또 기존의 value-function으로 estimated reward &lt;script type=&quot;math/tex&quot;&gt;\{y_{i, t}\}&lt;/script&gt;를 구할 수 있다. 결국 흔히 보듯이 L2 norm을 구해버리고 학습을 시키면 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathit{L}(\phi) = \frac{1}{2}\sum_{i}||V^{\pi}_{\phi}(s_{i}) - y_{i}||^{2}&lt;/script&gt;

&lt;p&gt;추가적으로 언급할 부분은 Advantage function을 구성할 때 bias-variance tradeoff 이슈가 생긴다. 만약 우리가 더 많은 variance를 허용할 수 없다면, 우리는 더 많은 실제 샘플을 추가해서 진행하면 된다. 그럴수록 우리는 더 많은 bias가 생길 수 밖에 없다. 이 부분은 &lt;script type=&quot;math/tex&quot;&gt;TD(\lambda)&lt;/script&gt;의 개념을 떠올리면 쉽게 이해가 갈 것이다.&lt;/p&gt;

&lt;p&gt;마지막으로 Actor-Critic의 개념은 Actor는 실제로 action을 담담하게 되는 Q-function 쪽을 의미하게 되고, critic은 기준점이 되는 V-function 을 의미하게 된다.&lt;/p&gt;

&lt;p&gt;참고&amp;gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic&quot;&gt;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/&quot;&gt;https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/&quot;&gt;http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Yongjin Shin</name></author><category term="Policy Gradients" /><category term="Actor-Critic Algorithms" /><summary type="html">논문읽다가 여전히 제대로 모르는 것 같아서 Berkeley 강의를 들었다. 매번 강의 듣고 치워버렸는데, 다른 공부하기 이전에 정리를 해보고 넘어가려고 한다. (생각 정리용)</summary></entry><entry><title type="html">좋은 연구란</title><link href="http://localhost:4000/research/talk/" rel="alternate" type="text/html" title="좋은 연구란" /><published>2019-09-01T00:00:00+09:00</published><updated>2019-09-01T00:00:00+09:00</updated><id>http://localhost:4000/research/talk</id><content type="html" xml:base="http://localhost:4000/research/talk/">&lt;p&gt;고맙게도 연구실원 중에 한 친구가 가끔씩 커피를 마시러 가자고 얘기를 해준다.
오늘도 논문 보면서 심란해하다가 이야기를 나누게 되었다.
다들 연차가 쌓여있는 친구들이라서 이해 안 되는 이야기를 할 때가 많지만, 그래도 도움이 되는 내용들이 항상 존재한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;실용성&quot;&gt;실용성&lt;/h3&gt;
&lt;p&gt;오늘은 좋은 논문에 대해서 이야기가 나왔다. 좋은 논문이란 뭘까에 대해서 석사하는 후배와 얘기를 나누었던 적이 있었는데, 그때 내가 내린 결론은 지식의 간극을 메꾸는 것이라고 (잠정적으로) 정리를 했었다.&lt;/p&gt;

&lt;p&gt;그런데 이번에 이 친구들을 통해서 새롭게 깨닫게 된 개념은 &lt;strong&gt;실용적&lt;/strong&gt;이라는 것이다. 
의미있는 논문(혹은 결과물)에 대한 고민은 어느 영역에나 있는 것인 것 같다.&lt;/p&gt;

&lt;p&gt;그런데 지식의 간극을 메우는 것은 연구(지식의 영역을 확장시켜 나가는 것이라 정의하겠다)를 좋은 연구가 되게 하는 하나의 요소이고,
실용적이라는 것은 “좋은” 연구에 대한 또 다른 해석일 수도 있을 것 같다.&lt;/p&gt;

&lt;p&gt;실용적이라 함은 쉽게 사용되고 널리 사용될 수 있는 것이라고 정의하면 될까?
어떤 역할을 하는 장치를 만들었을 때 그 어떤 모델에도 쉽게 끼워넣어서 좋은 결과를 얻을 수 있다면 실용적이라고 할 수 있는게 아닐까.
(그냥 현장에서 바로 가져다 쓸 수 있고 해석할 여지가 충분하고 등등..그냥 좋은 것)&lt;/p&gt;

&lt;h3 id=&quot;간단함&quot;&gt;간단함&lt;/h3&gt;
&lt;p&gt;Simple Stick. 스티브 잡스가 들고다니던 사랑의 회초리(응?).
예전에 단순함에 꽂혀있을 때, 읽었던 책에서 언급된 Simple stick은 이 연구라는 영역에서도 통용되는 말인듯 싶다.
앞서 언급된 실용적인 것에 더불어 &lt;strong&gt;간단, 명료&lt;/strong&gt; 역시 좋은 연구의 기준이 될 수 있을지 모르겠다.
어떻게든 꾸역꾸역 논문을 쓰는 것은 마치 어떻게든 꾸역꾸역 내 아이템으로 사업을 하는 것과 비슷하다.
될지 안될지 모르는 일이지만.&lt;/p&gt;

&lt;p&gt;그래서 그런지 안 되는 부분에서는 이것저것 다 붙여버리는 몽둥이처럼 휘두르는 경우가 있다. &lt;del&gt;그러면 사업이 안 좋게 된다&lt;/del&gt;
논문도 state of the art를 찍어보겠다고 이것 붙이고 저것 붙이고 그래서 왠지 모르게 originality가 모호한 논문이 있다.
과연 이러한게 나쁜 논문이라고 할 수는 없겠지만, 간단하게 딱! 하고 보여주는 논문이 더 좋은건 당연한게 아닐까?
(쓰는 사람이든 읽는 사람이든)
다만, 어려워서 문제이다. 더하는건 쉽지만 빼고 정제하는건 언제나 힘들다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;여담으로, 도메인을 찾아보자, 어려워도 처음부터 좋은 연구를 하도록 노력하자..라고 하더이다.
하, 아는데 쉽지 않다. 이 길이 맞을까? 이러다가 그냥 아웃풋도 없이 쫄딱 망하면 어쩌나? 하는 고민.
다른 분야에서 이리저리 떠다니다 여기와서도 똑같은 고민이다.
그냥 어딜가나 다 똑같나 보다. 친한 형이 말했듯이, 어떤 선택을 하는건 그다지 중요한게 아닌 것 같다.&lt;/p&gt;</content><author><name>Yongjin Shin</name></author><category term="연구" /><summary type="html">고맙게도 연구실원 중에 한 친구가 가끔씩 커피를 마시러 가자고 얘기를 해준다. 오늘도 논문 보면서 심란해하다가 이야기를 나누게 되었다. 다들 연차가 쌓여있는 친구들이라서 이해 안 되는 이야기를 할 때가 많지만, 그래도 도움이 되는 내용들이 항상 존재한다.</summary></entry><entry><title type="html">An Intro Lin.Prog &amp;amp; Game Theory-Chapter8</title><link href="http://localhost:4000/mas/game%20theory/lp-gt1/" rel="alternate" type="text/html" title="An Intro Lin.Prog &amp; Game Theory-Chapter8" /><published>2019-08-28T00:00:00+09:00</published><updated>2019-08-28T00:00:00+09:00</updated><id>http://localhost:4000/mas/game%20theory/lp-gt1</id><content type="html" xml:base="http://localhost:4000/mas/game%20theory/lp-gt1/">&lt;p&gt;Multi-agent Systems의 극악무도한 내용 때문에 다른 강의들 뒤적이다가 뭔가 LP를 다시 봐야할 것 같아서, 예전에 대학원 시험 때문에 공부했던 “An Introduction Linear Programming and Game Theory”를 다시 펼쳐봤습니다..신세계를 맛보았네요. 아주 아주 간략한 포인트만 정리해두도록 하겠습니다. (참고로 이 책에서는 오직 Two Person Zero Sum game만을 다루고 있어서, 결국 &lt;strong&gt;그&lt;/strong&gt; 책으로 돌아가야만 합니다..아..)&lt;/p&gt;

&lt;h3 id=&quot;82-some-principles-of-decision-making-in-game-theory&quot;&gt;8.2 Some Principles of Decision Making in Game Theory&lt;/h3&gt;
&lt;p&gt;Principle&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each player acts to maximize his security level.&lt;/li&gt;
  &lt;li&gt;The players tend to strategy pairs that are in equilibrium.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;간단하게, 각자 보수적인 선택을 하는 방향으로 갑니다. 어떻게 더 많이 얻을까가 아니라, 잃는데 얼머나 덜 잃을까를 선택하는 것입니다. 그리고 평형상태로 다가가는 것이지요. Principle 1은 Maxmin과 Minmax에 대한 idea를 던져주고 principle 2에서 그 유명한 &lt;script type=&quot;math/tex&quot;&gt;Maxmin \leq Minmax&lt;/script&gt;에 대한 얘기를 끄집어 냅니다.&lt;/p&gt;

&lt;h3 id=&quot;83-saddle-points&quot;&gt;8.3 Saddle Points&lt;/h3&gt;
&lt;p&gt;Thm&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;u_{1} \leq u_{2}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;U_{1} = u_{2}&lt;/script&gt; iff the payoff matrix A has a saddle point.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Def) An entry &lt;script type=&quot;math/tex&quot;&gt;a_{hk}&lt;/script&gt; of a payoff amtrix A is a saddle point if &lt;script type=&quot;math/tex&quot;&gt;Min_{1\leq j \leq n} a_{hj} = a_{hk} = Max_{1 \leq i \leq m} a_{ik}&lt;/script&gt;, that is, if &lt;script type=&quot;math/tex&quot;&gt;a_{hk}&lt;/script&gt; is the minimum of row h and the maximum of column k.&lt;/p&gt;

&lt;p&gt;뭔가 주저리 주저리 말이 많은데, 그냥 간단하게 Saddle Point 떠올리고 Maxmin과 Minmax의 안장점이다. 라고 하면되는데, Thm1의 증명은 직관적으로 생각할 수 있는데 책에 있는 매트릭스로 설명해놓은 부분이 너무 좋네요. (쉬우니깐)&lt;/p&gt;

&lt;h3 id=&quot;84-mixed-strategies&quot;&gt;8.4 Mixed Strategies&lt;/h3&gt;
&lt;p&gt;Def) Mixed Strategies for &lt;script type=&quot;math/tex&quot;&gt;P_{1}&lt;/script&gt; is a vector &lt;script type=&quot;math/tex&quot;&gt;X = (x_{1}, ... x_{m})&lt;/script&gt; of non-neg &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}&lt;/script&gt; satisfying probability axioms.&lt;/p&gt;

&lt;p&gt;Def) The expected payoff: &lt;script type=&quot;math/tex&quot;&gt;XAY^{T} = \sum_{1 \leq i \leq m} \sum_{1 \leq j \leq n} x_{i} a_{ij} y_{j}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Thm&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;v_{1} \leq v_{2}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;v_{1}&lt;/script&gt;은 그냥 행렬식으로 표현한 것. 귀찮으므로 생략)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;85-the-fundamental-theorem&quot;&gt;8.5 The Fundamental Theorem&lt;/h3&gt;
&lt;p&gt;아이디어 순서:&lt;/p&gt;

&lt;p&gt;8.4에서 나타낸 표기법들은 쫙 늘여써주면, Maxmin은 minization을 하는 primal problem이 되고, Minmax는 maximization을 하는 dual problem으로 나타나게 된다. 이때, payoff matrix가 0 이상일 경우에는 Duality Theorem에 의해서 반드시 finite solution 이 존재하게 된다! 이것이 결국 equilibrium이 된다. 음수가 포함된 payoff matrix에는 어떻게 잘 하면 된다고 하는데, &lt;del&gt;자세한 설명은 생략합니다&lt;/del&gt;&lt;/p&gt;</content><author><name>Yongjin Shin</name></author><category term="Book" /><category term="Summary" /><summary type="html">Multi-agent Systems의 극악무도한 내용 때문에 다른 강의들 뒤적이다가 뭔가 LP를 다시 봐야할 것 같아서, 예전에 대학원 시험 때문에 공부했던 “An Introduction Linear Programming and Game Theory”를 다시 펼쳐봤습니다..신세계를 맛보았네요. 아주 아주 간략한 포인트만 정리해두도록 하겠습니다. (참고로 이 책에서는 오직 Two Person Zero Sum game만을 다루고 있어서, 결국 그 책으로 돌아가야만 합니다..아..)</summary></entry><entry><title type="html">Multi-agent Systems-Chapter4</title><link href="http://localhost:4000/book/mas-chap4/" rel="alternate" type="text/html" title="Multi-agent Systems-Chapter4" /><published>2019-08-27T00:00:00+09:00</published><updated>2019-08-27T00:00:00+09:00</updated><id>http://localhost:4000/book/mas-chap4</id><content type="html" xml:base="http://localhost:4000/book/mas-chap4/">&lt;p&gt;3장에서는 게임에 있어서 최적 솔루션이 무엇인가(혹은 Nash Euilibrium) 이외에 다른 대안이 없는지에 대해서다. 4장에서는 그 솔루션을 얻는데 있어 계산이 얼마나 복잡할 것인가에 대한 문제를 다루게 된다. 자연스럽게 가장 간단한 2명의 플레이어, 제로섬 normal-form게임으로부터 시작하여, general-sum게임과 n플레이어+general-sum게임으로까지 확장된다. 더불어 3장에서 살펴보았던 maxmin/minmax전략과 우위전략, correlated equilbria 전략까지 솔루션을 얻는데 얼마나 복잡한지에 대한 탐구가 이어질 것이다.&lt;/p&gt;

&lt;p&gt;Side Notes:&lt;/p&gt;

&lt;p&gt;“Two-player zero-sum games are one of the few areas in game theory, and indeed in the social sciences, where a fairly shar, unique prediction is made” Robert Aumann 1987&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Indeed quilibria of zero-sum games are efficiently computable, comprise a convex set, can be reached via dynamics dfficiently.&lt;/li&gt;
  &lt;li&gt;While outside of zero-sum games equilibria are PPAD-hard, disconnected, and not reachable via dynamics&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;escape-1-approximation&quot;&gt;Escape 1: Approximation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;no player has no more than some small \eps incentive to deviate&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;escape-2-games-wspecial-strucure&quot;&gt;Escape 2: Games w/Special Strucure&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Arbitrary normal form are hard, but 2 player zeor sum aren’t
i.e. Multi-player Zero-sum Games, Anonymous Games&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;escape-3-alternative-solution-concepts&quot;&gt;Escape 3: Alternative Solution Concepts&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Two canonical and plausible alternatives: Correlated quilibrium, No-regret learning behaviour&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yongjin Shin</name></author><category term="MAS" /><category term="Summary" /><summary type="html">3장에서는 게임에 있어서 최적 솔루션이 무엇인가(혹은 Nash Euilibrium) 이외에 다른 대안이 없는지에 대해서다. 4장에서는 그 솔루션을 얻는데 있어 계산이 얼마나 복잡할 것인가에 대한 문제를 다루게 된다. 자연스럽게 가장 간단한 2명의 플레이어, 제로섬 normal-form게임으로부터 시작하여, general-sum게임과 n플레이어+general-sum게임으로까지 확장된다. 더불어 3장에서 살펴보았던 maxmin/minmax전략과 우위전략, correlated equilbria 전략까지 솔루션을 얻는데 얼마나 복잡한지에 대한 탐구가 이어질 것이다.</summary></entry><entry><title type="html">Ubuntu 18.04 설치</title><link href="http://localhost:4000/blog/tip1/" rel="alternate" type="text/html" title="Ubuntu 18.04 설치" /><published>2019-08-26T00:00:00+09:00</published><updated>2019-08-26T00:00:00+09:00</updated><id>http://localhost:4000/blog/tip1</id><content type="html" xml:base="http://localhost:4000/blog/tip1/">&lt;p&gt;간만에 연구실 출근해서 컴퓨터에 우분투18.04+윈도우10 설치를 했습니다. 원래 맥북을 써왔는데 연구실에서 윈도우를 쓰려니 도무지  적응이 안되어서 벼르고 있던 우분투를 깔아버렸네요.&lt;/p&gt;

&lt;p&gt;제대로 설치해보기는 처음인지라 한참을 헤매고 다녀서 오늘 하루 시간만 날려먹었네요. 설치에 도움이 되었던 자료들만 간략하게 정리를 해봅니다 (조만간 잘 못 만져서 갈아엎을 때 필요할 것 같으므로…)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;우분투 설치: &lt;a href=&quot;https://cupjoo.tistory.com/53&quot;&gt;https://cupjoo.tistory.com/53&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;고정 ip: &lt;a href=&quot;https://www.lesstif.com/pages/viewpage.action?pageId=61899302&quot;&gt;https://www.lesstif.com/pages/viewpage.action?pageId=61899302&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;한글 uim: &lt;a href=&quot;http://progtrend.blogspot.com/2018/06/ubuntu-1804-uim.html&quot;&gt;http://progtrend.blogspot.com/2018/06/ubuntu-1804-uim.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nvidia: &lt;a href=&quot;https://codechacha.com/ko/install-nvidia-driver-ubuntu/&quot;&gt;https://codechacha.com/ko/install-nvidia-driver-ubuntu/&lt;/a&gt; / &lt;a href=&quot;https://www.mvps.net/docs/install-nvidia-drivers-ubuntu-18-04-lts-bionic-beaver-linux/&quot;&gt;https://www.mvps.net/docs/install-nvidia-drivers-ubuntu-18-04-lts-bionic-beaver-linux/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;git: &lt;a href=&quot;https://www.crocus.co.kr/1494&quot;&gt;https://www.crocus.co.kr/1494&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;git client: Software App에서 다운로드. git kraken&lt;/p&gt;

&lt;p&gt;zsh: &lt;a href=&quot;https://dev.to/mskian/install-z-shell-oh-my-zsh-on-ubuntu-1804-lts-4cm4&quot;&gt;https://dev.to/mskian/install-z-shell-oh-my-zsh-on-ubuntu-1804-lts-4cm4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;vim: &lt;a href=&quot;https://norux.me/13&quot;&gt;https://norux.me/13&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pycharm: Software App에서 직접 다운로드&lt;/p&gt;

&lt;p&gt;conda: &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-install-the-anaconda-python-distribution-on-ubuntu-18-04&quot;&gt;https://www.digitalocean.com/community/tutorials/how-to-install-the-anaconda-python-distribution-on-ubuntu-18-04&lt;/a&gt;
zsh로하면 init이 안되므로 bashrc나 bash_profile에서 init 블락 copy&amp;amp;paste할 것
(pyenv: &lt;a href=&quot;https://lhy.kr/configuring-the-python-development-environment-with-pyenv-and-virtualenv&quot;&gt;https://lhy.kr/configuring-the-python-development-environment-with-pyenv-and-virtualenv&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;SVN: &lt;a href=&quot;https://lynnbaek.github.io/2018/10/11/ubuntu-rabbitvcs/&quot;&gt;https://lynnbaek.github.io/2018/10/11/ubuntu-rabbitvcs/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;spotify: &lt;a href=&quot;https://www.spotify.com/nl/download/linux/&quot;&gt;https://www.spotify.com/nl/download/linux/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;한글폰트: &lt;a href=&quot;https://syssurr.tistory.com/270&quot;&gt;https://syssurr.tistory.com/270&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;테마 변경: &lt;a href=&quot;https://codevkr.tistory.com/89&quot;&gt;https://codevkr.tistory.com/89&lt;/a&gt; / &lt;a href=&quot;https://logon.tistory.com/728&quot;&gt;https://logon.tistory.com/728&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Disable Beep sound: Zsh &lt;a href=&quot;https://blog.vghaisas.com/zsh-beep-sound/&quot;&gt;https://blog.vghaisas.com/zsh-beep-sound/&lt;/a&gt; Ubuntu &lt;a href=&quot;https://askubuntu.com/questions/1030515/turn-off-error-sound-on-ubuntu-18-04&quot;&gt;https://askubuntu.com/questions/1030515/turn-off-error-sound-on-ubuntu-18-04&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ssh 설정: &lt;a href=&quot;https://blog.lael.be/post/7678&quot;&gt;https://blog.lael.be/post/7678&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;프린터 설정: &lt;a href=&quot;https://tutorialforlinux.com/2018/04/21/driver-epson-l655-ubuntu-18-04-how-to-download-install/&quot;&gt;https://tutorialforlinux.com/2018/04/21/driver-epson-l655-ubuntu-18-04-how-to-download-install/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그 외 팁:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://brunch.co.kr/@calmglow/6&quot;&gt;https://brunch.co.kr/@calmglow/6&lt;/a&gt;&lt;/p&gt;</content><author><name>Yongjin Shin</name></author><category term="팁" /><category term="우분투" /><summary type="html">간만에 연구실 출근해서 컴퓨터에 우분투18.04+윈도우10 설치를 했습니다. 원래 맥북을 써왔는데 연구실에서 윈도우를 쓰려니 도무지 적응이 안되어서 벼르고 있던 우분투를 깔아버렸네요.</summary></entry><entry><title type="html">[번역] 성공하는 대학원생의 3가지 자질: 끈기, 고집 그리고 설득력</title><link href="http://localhost:4000/%EB%B2%88%EC%97%AD/Translation1/" rel="alternate" type="text/html" title="[번역] 성공하는 대학원생의 3가지 자질: 끈기, 고집 그리고 설득력" /><published>2019-08-25T00:00:00+09:00</published><updated>2019-08-25T00:00:00+09:00</updated><id>http://localhost:4000/%EB%B2%88%EC%97%AD/Translation1</id><content type="html" xml:base="http://localhost:4000/%EB%B2%88%EC%97%AD/Translation1/">&lt;p&gt;대학원 생활을 시작하기 직전에 Matt Might 교수님의 블로그를 우연하게도 접하게되어서 글을 찬찬히 읽어보니 좋은 글들이 많이 있네요.&lt;/p&gt;

&lt;p&gt;사실 블로그 시작해야지 하다가 이 글의 가장 마지막 문단을 통해 블로그를 만들어버렸습니다.
번역 오류나 부자연스러운 부분에 대한 지적은 언제든지 환영합니다.&lt;/p&gt;

&lt;p&gt;원문: &lt;a href=&quot;http://matt.might.net/articles/successful-phd-students/&quot;&gt;http://matt.might.net/articles/successful-phd-students/&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;매년 가을, 대학원 신입생들이 온다.
나는 신입생들을 받고 있기 때문에, “박사 졸업은 얼마나 걸리나요?”라는 질문을 매년 받는다.
나는 “대학원은 자네가 원하는 만큼 오래 다닐 수 있네”라고 답한다. 졸업에 필요한 자격 요건을 갖추는데에는 속도제한 따위는 없기 때문이다. 그러나 이 질문은 옳은 질문이라 할 수 없다.&lt;/p&gt;

&lt;p&gt;더 나은 질문은 “무엇이 성공하는 대학원생이 되도록 만드나요?” 이다.
4개의 대학에서 학생들의 성공과 실패를 지켜보면서, 나는 3가지 자질이 성공적인 대학원 생활을 이끈다고 본다: 끈기, 고집 그리고 설득력이다.&lt;/p&gt;

&lt;p&gt;만약 당신이 대학원생이거나 혹은 대학원에 진학하고자 한다면, 계속 읽어보기를 바란다.&lt;/p&gt;

&lt;h2 id=&quot;그다지-관계-없는-요인&quot;&gt;그다지 관계 없는 요인&lt;/h2&gt;

&lt;p&gt;‘박사는 똑똑 해야만 한다.’ 라는 것은 터무니 없는 오해이다. 이것은 명백히 잘못된 생각이다. 똑똑한 학생들은 박사 졸업장을 가지는 것보다 더 많은 것을 알고 있을 것이다.&lt;/p&gt;

&lt;p&gt;명석함이나 머리회전이 빠른 것 같이 ‘똑똑함’이라는 것은 대학원과 관계가 멀다. 오로지 명석함과 빠른 머리 회전으로 지금까지 살아온 학생이라면 대학원 과정에서 분명히 떨어져나갔을 것이다. 물론, 다른 직업군에서 그러한 능력들은 가치가 있는 것은 분명하다. 그러나 과학에 있어서 그러한 자질이 필요조건 혹은 충분조건이 되지는 않는다.&lt;/p&gt;

&lt;p&gt;분명히 똑똑한 것은 도움이 된다. 그러나 그것이 전부가 아니다. 박사를 졸업한 사람들 아무나 붙잡고 물어봐도 수긍을 할 것이다. 박사 졸업장을 무사히 가져간 사람들이 꼭 1등급 학생들은 아니라는 것을 말이다.&lt;/p&gt;

&lt;p&gt;나의 지도 교수님은 종종 이런 말씀을 하셨다. “내가 대학원생일때, 우울증이 도지고 아마 박사를 마치지 못할 것 같다 라고 생각이 들 때 마다 나는 나보다 멍청한 졸업생들을 떠올렸다. 그리고 스스로 되뇌었지. ‘저 바보도 졸업을 하는데, 나도 할 수 있다’”&lt;/p&gt;

&lt;p&gt;내가 교수가 된 이후로, ‘졸업’이라는 말 대신 ‘펀딩 받기’라는 말로 대체하여 내 지도 교수의 말을 나 스스로 곱씹는 것을 알게되었다. (이 문장을 적고 한 달 안에 나는 처음으로 세 개의 펀딩을 받게 되었다.)&lt;/p&gt;

&lt;h2 id=&quot;끈기&quot;&gt;끈기&lt;/h2&gt;

&lt;p&gt;박사 졸업장을 따내기 위해선, 당신은 반드시 인류가 가진 지식의 영역을 유의미하게 넓혀야만 한다. 더 정확하게는, 당신이 한 연구가 지식의 영역을 확장시켰다는 점을 최전방에 있는 전문가들에게 납득시켜야 한다는 것이다.&lt;/p&gt;

&lt;p&gt;당신은 수업을 듣고 논문을 읽으면서 지식의 영역이 어디까지 펼쳐져있는지 알 수 있다. 이것은 쉬운 일이다. 그러나 지식의 영역을 확장시켜나간다는 것은 마치 실패라는 적의 공습에 대비해 벙커에 들어가 있는 것과도 같다.&lt;/p&gt;

&lt;p&gt;지식의 전위에 도달한 많은 대학원생들은 우울함에 빠지게 되는데, 왜냐하면 더 이상 그들을 몰아붙이던 시험이라는 것이 없어지기 때문이다. 이때(2-3년차) 가장 힘든 시기이다.&lt;/p&gt;

&lt;p&gt;해결하고자 하는 문제를 찾는 것이 문제가 되는 경우는 거의 없다. 모든 영역에 있어서 답이 없는 문제들은 차고 넘친다. 만약 문제를 찾는 것이 어렵다 라고 한다면, 아마 당신은 잘못된 영역에 있을 가능성이 크다. 진짜 어려운 부분은 문제가 없는 문제의 해결책을 찾는 것이다. 물론, 누군가가 그것을 어떻게 푸는지 당신에게 알려준다면, 더 이상 답이 없는 문제가 아니다.&lt;/p&gt;

&lt;p&gt;이 시기를 견뎌내기 위해서는, 당신은 침대에서 일어나고 다시 잠들기까지 그 모든 순간에 기꺼이 실패를 마주할 수 있어야 한다. 당신은 하루가 되었든, 한 달이 되었든, 심지어 일년이 되었든, 그 시간들이 수포로 돌아간다고 해도 그 실패를 받아들일 준비가 되어야 한다. 이 고난의 시간에 당신이 가꾸는 능력이야말로 그럴듯한 해결책을 상상하도록 하고, 그리고 접근법이 작동할 것인지에 대한 감을 갖추게 할 것이다.&lt;/p&gt;

&lt;p&gt;만약 당신이 이 시기를 끈질기게 견뎌내기만 한다면, 당신은 이전에는 가지지 못했던 직관으로 문제의 해답을 구할 것이다. 당신은 아마 당신이 갑자기 어떻게 그런 해결책을 떠올리는지 알 수 없을 것이다 (나도 내가 어떻게 이런 직관이 생겨난지 모른다.). 그냥 그렇게 될 것이다.&lt;/p&gt;

&lt;p&gt;당신이 이 능력을 습득하게 될 때, 당신은 아직은 미숙한 논문을 리뷰어에게 보낼 것이고 다른 사람들은 당신의 연구에 대해 어떻게 평가할 것인지를 보게될 것이다. 좋은 학회는 8~25%의 논문들만 수용되므로, 당신이 제출한 거의 대부분의 논문은 탈락할 것이다. 결국에는 당신은 당신의 연구가 어떻게하면 출간될 수 있을지에 대해서 알아낼 수 있기를 희망할 뿐일 것이다. 만약 이러한 자세를 유지하고 정말로 열심히 연구를 한다면, 당신은 결국 그렇게 될 것이다.&lt;/p&gt;

&lt;p&gt;학부 때 뛰어난 학생들은 갑작스럽게도 끊임없이 맛보게되는 실패가 달갑지 않을 것이다. 만약 당신이 자존심이 굉장히 세다면, 대학원 과정은 이를 고쳐줄 것이다. 굉장히 격렬하게.&lt;/p&gt;

&lt;p&gt;불확실성과 실패 그리고 좌절을 동반하는 대학원 과정의 중요한 순간에는 끈기가 요구된다.&lt;/p&gt;

&lt;h2 id=&quot;고집&quot;&gt;고집&lt;/h2&gt;

&lt;p&gt;대학원 이후 정교수 자리를 따내기 위해서는, 당신은 또다른 자질이 필요하다: 고집스러움이다. 정교수자리는 얼마되지 않기 때문에 이를 차지하기위해서는 치열한 경쟁이 생긴다.&lt;/p&gt;

&lt;p&gt;컴퓨터 공학에서는 경쟁력있는 교수 후보자가 되기 위해서 10개의 publication와 그 중 3-5개는 탑티어급의 논문이어야 한다. 박사 졸업장을 취득했다고 해서 취업이 바로 되는 것은 아니다.&lt;/p&gt;

&lt;p&gt;박사과정을 하는 좋은 이유는 그렇게 많지 않다. “나는 교수가 되고 싶다”라는 것이 유일할지도 모른다.대학원 생활이 끝날 때까지 당신이 교수가 되고 싶다라는 것을 깨닫지 못하게 하는 좋은 기회들이 (아이러니하게도) 있을지도 모른다. 그러니 만약 당신이 대학원에 가고 싶거든, 제대로 생각을 해보길 바란다.&lt;/p&gt;

&lt;p&gt;교수가 되는 것은, 답이 없는 어떤 한 개의 문제를 풀거나 미지의 한 영역에 대해서 연구하는 것 이상이다. 당신은 여러 가지 문제와 영역에서 해답을 내놓아야만 한다. 대학원을 졸업하면, 당신이 연구한 결과들을 잇는 연결선들이 점점 들어나야만 하고 이것을 통해서 교수들에게 당신이 하고 있는 연구들이 의미있게 나아가고 있음을 증명해내야만 한다.&lt;/p&gt;

&lt;p&gt;또한, 당신은 당신이 연구하는 분야의 사람들과 활발하게 (심지어 공격적으로) 관계를 맺어나가야 한다. 그 곳의 연구자들은 당신이 누구이고 당신이 무슨 연구를 하는지에 대해 알 필요가 있다. 뿐만 아니라 그들이 당신에 대해서 관심을 가지도록 해야 한다.&lt;/p&gt;

&lt;p&gt;이러한 것들은 한순간에 뿅하고 만들어지지는 않을 것이다.&lt;/p&gt;

&lt;h2 id=&quot;설득력&quot;&gt;설득력&lt;/h2&gt;

&lt;p&gt;마지막으로 좋은 대학원생은 분명하고 당당하게 자신의 아이디어 대해서 전달할 수 있는 능력을 갖추고 있다. 사람에게 직접 전달하든 글로써 전달되든 것에 상관없이 말이다.&lt;/p&gt;

&lt;p&gt;과학이라는 것은 발견을 하는 행위임과 동시에 설득을 하는 행위이다.&lt;/p&gt;

&lt;p&gt;일단 당신이 무언가를 발견했다면, 당신의 결과물이 논리적이고, 의미있는 기여가 있다라는 것을 전문가들에게 설득을 해야만 한다. 이것은 보기보다 꽤나 힘든 일이다. 그냥 단순히 “데이터”만 보여주는 것은 전혀 도움이 되지 못한다. (이상적인 세상에서는 이것이 충분하지만 말이다.)&lt;/p&gt;

&lt;p&gt;대신 당신은 전문가들에게 떠먹여주다시피 해야만 한다. 당신이 논문을 쓸 때에는, 당신의 발견으로 인해 그들이 가지게 되는 인지적 고통과 받아들이는 시간을 최소화 시킬 수 있는 방향으로 나가야 한다.&lt;/p&gt;

&lt;p&gt;당신은 어쩌면 “출장”이라는 명목하에 발표를 하고 당신의 연구를 통해 사람들이 희열을 느끼게 해줘야 할 지도 모른다. 당신이 컨퍼런스에서 이야기를 할 때, 당신은 청자들이 다음 내용이 뭘까에 대해서 열광적으로 기다리기를 원할 것이다.&lt;/p&gt;

&lt;p&gt;당신은 이목을 끌 수 있는 초록(abstract)을 작성해야만 하고, 개요에서 독자들이 충분히 시간을 들여 읽어볼만하게끔 만들어둬야 한다. 당신은 명쾌함(Clarity)과 엄밀함(Precision) 사이의 균형을 잡는 법을 익혀야 한다. 이를 통해서 당신의 아이디어는 모호하지도, 그렇다고 숨막히게 딱딱하지도 않는 그 중간에 있어야만 한다.&lt;/p&gt;

&lt;p&gt;일반적으로, 신입생들은 커뮤니케이션에 그닥 익숙하지 못한채 입학을 한다. 이것 역시 당신이 대학원에서 갈고 닦아야하는 스킬이다. 이것을 빨리 얻을수록, 더 좋은 것은 자명하다.&lt;/p&gt;

&lt;p&gt;불행하게도, 글쓰기를 잘하는 유일한 방법은 많이 쓰는 것 뿐이다. 어떤 것에서 전문가가 되기 위해서는 10,000시간이라는 마법과도 같은 법칙이 있다. 당신은 논문을 쓰면서 10,000시간 글쓰기에 절대로 도달할 수 없을 것이다. 6년동안 매일 5시간씩 글을 쓴다면 대학원 졸업 전까지 10,000시간 글쓰기를 할 수 있을 것이다. (대학원 생활을 통틀어 하루에 12시간 글을 쓰는 것은 굉장히 드문 일이다.)&lt;/p&gt;

&lt;p&gt;그렇기 때문에 나는 신입생들이 오면 블로그를 시작하라고 한다. 아무도 읽지 않을지라도 일단 하라고 조언한다. 꼭 그들의 연구가 아닐지라도 말이다. 글 쓰는 연습을 하는 것 자체가 중요하기 때문이다.&lt;/p&gt;</content><author><name>Yongjin Shin</name></author><category term="번역" /><summary type="html">대학원 생활을 시작하기 직전에 Matt Might 교수님의 블로그를 우연하게도 접하게되어서 글을 찬찬히 읽어보니 좋은 글들이 많이 있네요.</summary></entry></feed>