---
title: "Revisit Knowledge Distillation: A teacher-free framework"
usemathjax: true
toc: true
toc_sticky: true

categories:
- review
tags:
- Knowledge Distillation
- Label Smoothing
---



Knowledge Distillation을 Label Smoothing의 uniform Distribution 대신 Learned Distribution으로 해석하였음.



## 요약

* KD는 결과적으로 Label Smoothing에서 Uniform대신에 학습된 분포를 사용한다는 점에서 달라진다. 그런데, Label Smoothing을 했을 때 KD는 잘 안 된다. [^1]  
* soft targeting을 이용한 KD는 특별한 종류의 label smoothing (uniform대신에 teacher의 지식을 이용한)이라고 할 수 있겠다. 그럼 KD를 위한 soft targeting은 반면에 overconfidence 문제를 잘 해결해주는가? 어쩌면? KD는 좀 더 confidence한 방면으로 나가야만 즉, 클래스 간의 거리를 좁히거나 넓히는 작용을 해야만 가능하기 때문이다. Regularizing Term이라고 볼 수도 있겠지만 그럼에도 불구하고 그것은 클래스 사이의 관계를 여전히 반영하고 있기 때문이다.
* 근데 Regularizer로 Re-KD와 De-KD는 어떻게 설명이 가능한거지?? 제대로 학습이 안 된 경우라 할지라도 Regularizer Term으로 사용되기 때문에 좀 더 Robust한 모델로 학습이 가능하다는 점일까? 맞나?
* Tf-KD_self는 동일한 구조의 모델에서 미리 학습을 한 다음, 그걸로 동일한 구조의 모델을 다시 학습시킨다는 것임. Tf-KD_reg의 경우는 Label Smoothing에서 Uniform 대신에 조금 더 나은 분포를 이용해서 KD를 하겠다는 것. 이게 되는 이유는 KD에서 학습되는 방식이 Regularizer로 생각하기 시작했기 때문.



## 생각

* 근데 다시 질문해야하는 것은, 그래서 어떤 Regularizer를 쓰는게 좋은가. 과연 KD를 하게 만드기만 한다고 해서 좋은 것인가? LBR은 overconfidence를 막을 수 있는 "좋은" 모델을 만드는데 저렇게 학습된 KD는 그런 "좋은" 모델인가?





[^1]: 그것은 Label Smoothing에서 Uniform으로 사용되기 때문에 충분히 Regularizing을 못해주기 때문이다. Learned Probability는 데이터마다 다른 거리를 가지게 함으로써 뭔가 클래스마다 다른 거리를 가지게 만든다. 따라서 어느 정도의 Class 별로 거리도 지닐 수도 있지만, 그것이 단순히 Target을 쫓지 못하도록 class별로 차이를 어느정도 일정하게 유지시키는 힘을 가지게 된다.

