---
title: "The Non iid Data Quagmire of Decentralized Machine Learning"
usemathjax: true
toc: true
toc_sticky: true

categories:
- review
tags:
- non iid
- federated learning
- model traveling
---

FL에서는 non-iid 문제를 어떻게 해결하도록 시도할까에 대한 첫번째 논문. 근데 방법론이 별로 대단한거 같지는 않는데 설명이 불친절한지 내가 멍청한건지 감이 안옴.

[모델이름: SkewScout | 저자: Kevin Hsieh et al. 2019. ](https://arxiv.org/abs/1910.00189)



## 요약





## 서론

* 기존 FedAvg 같은 논문들은 커뮤니케이션을 줄이는데만 집중했기 때문에, iid 가정이라든지 non-iid 조건을 약하게 걸어서 결과를 도출했다. 실험적으로 알아낸 사실은 (2번 빼곤 당연한 이야기이긴 한데)

1. non-iid 세팅에서 FedAvg, Gaia, Deep Gradient Compression, BSP는 모델에 따라서(AlexNet, GN-LeNet) 성능이 안 좋아지는 결과를 내는데, 이는 non-iid가 기존의 방법에서는 accuracy를 감소시킬 수 밖에 없음을 나타냄.
2. Batch Normalization은 non-iid 세팅에 굉장히 민감하고, 이에 반해 Group Normalization은 괜찮은 성능을 보여준다 (그렇다고 해결책이란 말은 아님).
3. iid에서 많이 벗어날수록 테스크는 더 어려워진다.



## 방법론

* 근본적인 아이디어는 iid에 많이 벗어나는 partition을 찾아내서 거기서 계속 학습하면서 general model을 업데이트 시키고, 커뮤니케이션에서 accuracy loss가 도달해서 커지는 부분들에서 model이 이동(한다고 설명을 하기는 하지만 그냥 그쪽의 local model에 가중치를 1로 둔다고 생각하면 되는건가..?)



## 생각




