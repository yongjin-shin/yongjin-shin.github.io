---
title: "Transfer Learning with a mixture of self-supervised experts"
usemathjax: true
toc: true
toc_sticky: true

categories:
- review
tags:
- Federated Learning
- Transfer Learning
- mixture model
---



Federated Learning과 유사한 환경에서 Transfer Learning에서 Active Learning과 유사하게 Data를 가져오는 문제를 풀고자 하였음[^1]

![tl_fl](/assets/images/2020-03-20-fd-aug%20(copy)/tl_fl.png)



## 요약

* 문제는 Federated Learning과 유사하다면 유사할수도 있는데, Decentralized Learner가 각기 다른 환경에서 Central Server와 communication을 통해 학습을 진행하는 것이다.
* 그런데, FL에서는 Server가 데이터를 가지고 있지 않다면, 여기에서는 Server에서 추가적인 데이터를 왕창 가지고 있다는 점이다. 그리고 Learners는 Server에 어떤 데이터를 달라고 할지를 요청해야 하는 상황으로 바뀌었다.
* 여기서 요청을 한다는 점에서 Active Learning 적인 요소가 들어와있으나, 본 논문에서는 직접적인 비교를 하지 않았고 따라서 Reviews도 그 부분에서 있어서 부족하다고 지적하였다.
* Base Model은 Server에서 해당 데이터들을 이용해서 학습이 이루어진다. Pre-Trained Base Model은 모든 Learners에게로 전달이 되고, Local의 Learner는 자기가 가지고 있는 데이터와 비교를 하여 Central에 있는 데이터 중 어느 데이터를 추가로 가져와야할지를 결정하게 된다.
* 이때 Base Model은 본 논문의 제목과 같이 Mixture of Experts 형식으로 학습이 된다.



## 생각

* Federated Learning에서는 왜 Ensemble처럼 학습을 하지 않을까? 
  * Model Parameter들 합치기도 힘든데, 그냥 $$\lambda$$를 잘 조절해줄 수 있는 Gate만 잘만 학습시켜준다면 충분히 가능하지 않을까?? 
  * Non-parametric과 Mixture of Experts라면 충분히 학습이 가능하지 않을까? 
  * 어차피 local에 있는 Learner는 그렇게 크게 만들기도 힘들지 않나? Ensemble로 하였을 때 뭐가 문제일까? 
  * local로 보낼 때 모델이 너무 커진다라는 단점? 그러면 조금 선택적으로 보내면 안되나? 선택을 어떻게 하지?
  * 여기서 논문처럼 Active Learning과 같은 signal을 보내고, 그걸 통해 model을 걸러낼까?



## Reference

[^1]: https://openreview.net/forum?id=r1lI3ertwH