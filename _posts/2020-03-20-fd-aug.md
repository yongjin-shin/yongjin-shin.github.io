---
title: "Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data"
usemathjax: true
toc: true
toc_sticky: true

categories:
- review
tags:
- Federated Learning
- Knowledge Distillation
- GAN
---



Federated Learning에서 Knowledge Distillation으로 Communication Burden을 줄임과 동시에 GAN을 사용해서 Non-i.i.d를 해결하고자 하였다.

![fd-aug](/assets/images/2020-03-16-tf-kd%20(copy)/fd-aug.png)



## 생각

* 나 이외의 다른 agent의 logit의 평균값을 knowledge distillation의 Teacher로 사용하였다(global-average logit vector). 그리고 local에서도 각각의 label마다의 logit의 평균을 이용하여 하나의 값을 도출하였다(local-average logit vector).
* Non-i.i.d를 해결하기 위해서는 중앙서버의 GAN에 각 agent가 일부 데이터를 샘플링하여 전달한 다음 서버에서 학습을 하고, GAN 모델을 다시 local에 전달을 한다. 이 때, 샘플링하는 데이터는 



## 질문

* 근데 다시 질문해야하는 것은, 그래서 어떤 Regularizer를 쓰는게 좋은가. 과연 KD를 하게 만드기만 한다고 해서 좋은 것인가? LBR은 overconfidence를 막을 수 있는 "좋은" 모델을 만드는데 저렇게 학습된 KD는 그런 "좋은" 모델인가?




